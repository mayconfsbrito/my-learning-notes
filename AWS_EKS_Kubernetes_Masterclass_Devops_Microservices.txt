
AWS EKS Kubernetes Masterclass DevOps Microservices

1st Repo:
https://github.com/stacksimplify/aws-eks-kubernetes-masterclass

01-02-Create-EKSCluster-and-NodeGroups/
https://github.com/stacksimplify/aws-eks-kubernetes-masterclass/tree/master/01-EKS-Create-Cluster-using-eksctl/01-02-Create-EKSCluster-and-NodeGroups
1 - Create EKS cluster using eksctl
	> eksctl create cluster ...
2 - Create e associate IAM OIDC Provider for the EKS Cluster
	> eksctl utils associate-iam-oidc-provider ...
3 - Create a new keypair
4 - Create a Node Group with Add-Ons in Public Subnets
	> eksctl create nodegroup ...
5 - Verify cluster and nodes
6 - Update Worker Nodes security group to allow all traffic


01-04-Delete-EKSCluster-and-NodeGroups
https://github.com/stacksimplify/aws-eks-kubernetes-masterclass/tree/master/01-EKS-Create-Cluster-using-eksctl/01-04-Delete-EKSCluster-and-NodeGroups
1 - Delete Node Group
	> eksctl get clusters
	> eksctl get nodegroup --cluster=<clusterName>
	> eksctl delete nodegroup --cluster=<clusterName> --name <nodeGroupName>
2 - Delete Cluster
	> eksctl delete cluster <clusterName>


Docker Fundamentals
https://github.com/stacksimplify/docker-fundamentals


Kubernetes Fundamentals
https://github.com/stacksimplify/kubernetes-fundamentals
02-PODs-with-kubectl
1. Create Pod
	> kubectl run my-first-pod --image stacksimplify/kubenginx:1.0.0
2. List Pods with Wide option
	> kubectl get pods -o wide
3. Describe pod
	> kubectl describe pod <Pod-Name>
3. To access a pod externaly we need to create a NodePort Service
	- Ports:
		- port: Port on which node port service listens in Kubernetes cluster internally
		- targetPort: We define container port here on which our application is running
		- NodePort: Worker node port on which we can access our application
	> kubectl expose pod <Pod-Name>  --type=NodePort --port=80 --name=<Service-Name>
4. Get Public IP of Worker Nodes and how to access it
	> kubectl get nodes -o wide
	> http://<node1-public-ip>:<Node-Port>
4. Verify pod logs
	> kubectl get po
	> kubectl logs <pod-name>
5. Connect to Container in POD
	> kubectl exec -it <pod-name> -- /bin/bash
6. Get Pod or Service definition YAML output
	> kubectl get pod my-first-pod -o yaml
	> kubectl get service my-first-service -o yaml

03-ReplicaSets-with-kubectl
https://github.com/stacksimplify/kubernetes-fundamentals/tree/master/03-ReplicaSets-with-kubectl
ReplicaSet
	- Purpose: Maintain a stable set of replica Pods running at any given time
	- If our application craches (any pods die)
		- Replicaset will recreate the pod immediately to ensure the configured number of pods running at any given time
	- Kubernetes provides pod load balancing out of the box using Services for the pods which are part of a ReplicaSet
	- Labels & Selectors:
		- Are the key items which ties all 3 together (Pod, ReplicaSet & Service)
	- Scaling: Kubernetes enables us to easly scale up our application, adding additional pods as needed
1. List ReplicaSets
	> kubectl get replicaset
	> kubectl get rs
2. Expose ReplicaSet as a Service
 	> kubectl expose rs <ReplicaSet-Name>  --type=NodePort --port=80 --target-port=8080 --name=<Service-Name-To-Be-Created>
3. Apply latest changes to ReplicaSet
	> kubectl replace -f <File-Name>

04-Deployments-with-kubectl/
https://github.com/stacksimplify/kubernetes-fundamentals/tree/master/04-Deployments-with-kubectl
Deployment
	- Create a Deployment to rollout a ReplicaSet
	- Deploytment -> ReplicaSet -> Pods
1. Create Deployment
	> kubectl create deployment <Deplyment-Name> --image=<Container-Image>
2. Scaling a Deployment
	> kubectl scale --replicas=20 deployment/<Deployment-Name>
3. Expose Deployment as a Service
	> kubectl expose deployment <Deployment-Name>  --type=NodePort --port=80 --target-port=80 --name=<Service-Name-To-Be-Created>
4. Update image of a Deployment
	> kubectl set image deployment/<Deployment-Name> <Container-Name>=<Container-Image> --record=true
5. Verify Rollout Status
	> kubectl rollout status deployment/my-first-deployment
6. Describe Deployment (to verify and understand that K8s by default do "Rolling Update" for new application releases)
	> kubectl describe deployment <Deployment-Name>
7. Verify Rollout history of a Deployment
	> kubectl rollout history deployment/<Deployment-Name>
8. Edit Deployment
	> kubectl edit deployment/<Deployment-Name> --record=true
	- Change the yaml file
9. Check the history and Verify changes in each version
	> kubectl rollout history deployment/<Deployment-Name>
	> kubectl rollout history deployment/<Deployment-Name> --revision=1
10. Rollback a Deployment to previous version
	> kubectl rollout undo deployment/<Deployment-Name>
11. Rollback to specific revision
	> kubectl rollout undo deployment/<Deployment-Name> --to-revision=3
12. Pause Deployment
	- If we need to do multiple changes in our deployment
		- We can pause the the deployment and make all changes and resume it
	> kubectl rollout pause deployment/<Deployment-Name>
	- And make multiple changes, like these:
		- Update the application version from V3 to V4
			> kubectl set image deployment/<Deployment-Name> kubenginx=stacksimplify/kubenginx:4.0.0 --record=true
		- Make one more change, changing resources for the Deployment
			> kubectl set resources deployment/<Deployment-Name> -c=kubenginx --limits=cpu=200m,memory=512Mi
13. After the pause and changes, Resume the Deployment
	> kubectl rollout resume deployment/<Deployment-Name>

05-Services-with-kubectl
https://github.com/stacksimplify/kubernetes-fundamentals/tree/master/05-Services-with-kubectl
- ClusterIP:
	- Used for communication between applications inside k8s cluster
	- ex: Frontend application accessing backend application
- NodePort:
	- Used for accessing applications outside of k8s cluster using Worker Node Ports
	- ex: Accessing Frontend application on browser
- LoadBalancer:
	- Primarily for Cloud Providers to integrate with their Load Balancer services
	- ex: AWS Elastic Load Balancer
- Ingress:
	- Ingress is an advanced load balancer which provides Context path based routing, SSL, SSL Redirect and many more
	- ex: AWS ALB
- externalName:
	- To access externally hosted apps in kk8s cluster
	- ex: Access AWS RDS Database endpoint by application present inside k9s cluster
- Example deploying a cluster and exposing Services
	- One ClusterIP Service to expose the Backend App, letting the Frontend app to consume that
	- One NodePort Service to expose de FrontendApp externally (to users)
	1. Create Deployment
		> kubectl create deployment my-backend-rest-app --image=stacksimplify/kube-helloworld:1.0.0
	2. Create ClusterIP Service for Backend Rest App
		> kubectl expose deployment my-backend-rest-app --port 8000 --target-port 8080 --name=my-backend-service
	3. Create your own reverse proxy
		- https://github.com/stacksimplify/kubernetes-fundamentals/tree/master/00-Docker-Images/03-kube-frontend-nginx
	4. Create Deployment for Frontend Nginx Proxy
		> kubectl create deployment my-frontend-nginx-app --image=stacksimplify/kube-frontend-nginx:1.0.0
		> kubectl get deploy
	5. Create NodePort Service for Frontend Nginx Proxy
		> kubectl expose deployment my-frontend-nginx-app  --type=NodePort --port=80 --target-port=80 --name=my-frontend-service
		> kubectl get svc
	6. Capture IP and Port to Access Application
		> kubectl get svc
		> kubectl get nodes -o wide
		> http://<node1-public-ip>:<Node-Port>/hello
	7. Scale backend with 10 replicas
		> kubectl scale --replicas=10 deployment/my-backend-rest-app
	8. Test again to view the backend service Load Balancing
		> http://<node1-public-ip>:<Node-Port>/hello

aws-eks-kubernetes-masterclass (AGAIN)
04-EKS-Storage-with-EBS-ElasticBlockStore
https://github.com/stacksimplify/aws-eks-kubernetes-masterclass/tree/master/04-EKS-Storage-with-EBS-ElasticBlockStore
04-01-Install-EBS-CSI-Driver/
1. Create IAM policy
2. Get the IAM role worker nodes using and associate this policy to that role
	> kubectl -n kube-system describe configmap aws-auth
	- from output check rolearn
	- Go to Services > IAM > Roles > Search for the returned role and open it
		- Click in Permissions > Attach policies > Search and attach the create
		policy in the step 1
5. Deploy the Amazon EBS CSI Driver
	- Check if the k8s version it's greather than 1.14
		> kubectl version --client --short
	- Deploy EBS CSI Driver
		> kubectl apply -k "github.com/kubernetes-sigs/aws-ebs-csi-driver/deploy/kubernetes/overlays/stable/?ref=master"
	- Verify ebs-csi pods running
		> kubectl get pods -n kube-system

04-02-SC-PVC-ConfigMap-MySQL
https://github.com/stacksimplify/aws-eks-kubernetes-masterclass/tree/master/04-EKS-Storage-with-EBS-ElasticBlockStore/04-02-SC-PVC-ConfigMap-MySQL
* Create a MySQL Database with persistence storage using AWS EBS Volumes
1. Create Storage Class & PVC
	> kubectl apply -f kube-manifests/
	- Get the files on kube-manifests folder on the link above
	- Check these files to understand better about this workload
2. List Storage Classes
	> kubectl get sc
3. List PVC
	> kubectl get pvc
4. List PV
	> kubectl get pv
5. Connect to MySQL Database (Test)
	> kubectl run -it --rm --image=mysql:5.6 --restart=Never mysql-client -- mysql -h mysql -pdbpassword11
	> show schemas;

04-03-UserManagement-MicroService-with-MySQLDB
https://github.com/stacksimplify/aws-eks-kubernetes-masterclass/tree/master/04-EKS-Storage-with-EBS-ElasticBlockStore/04-03-UserManagement-MicroService-with-MySQLDB
1. Create user management rvice deployment and service
2. Create Deployment & NodePort Service
	> kubectl apply -f kube-manifests/
3. List Pods
	> kubectl get pods
4. Verify logs of Usermgmt Microservice pod
	> kubectl logs -f <Pod-Name>
5. Verify sc, pvc, pv
	> kubectl get sc,pvc,pv
- Problem observation:
	- Pod will be restarting multiple times due to unavailability of Database
	- To avoid this, we can apply initContainer concept to our Deployment manifest

05-Kubernetes-Important-Concepts-for-Application-Deployments
05-02-Kubernetes-Init-Containers
https://github.com/stacksimplify/aws-eks-kubernetes-masterclass/tree/master/05-Kubernetes-Important-Concepts-for-Application-Deployments/05-02-Kubernetes-Init-Containers
- Init Containers run before App Containers
- Init Containers can contain utilities or setup scripts not present in an app image
- We can have and run multiple Init Containers before App Container
- Init containers are exactly like regular containers, except:
	- Init containers always run to completion
	- Each init container must complete successfully before the next one starts
- If a Pod's init container fails, K8s repeatedly restarts the Pod until the init container succeds
- However, if the Pod has a restartPolicy of Never, Kubernetes does not restart the Pod

05-03-Kubernetes-Liveness-and-Readiness-Probes
https://github.com/stacksimplify/aws-eks-kubernetes-masterclass/tree/master/05-Kubernetes-Important-Concepts-for-Application-Deployments/05-03-Kubernetes-Liveness-and-Readiness-Probes

05-04-Kubernetes-Requests-Limits
https://github.com/stacksimplify/aws-eks-kubernetes-masterclass/tree/master/05-Kubernetes-Important-Concepts-for-Application-Deployments/05-04-Kubernetes-Requests-Limits
- Kubelet reserves at least the request amount of the system resource specifically for that container to user
- List Pods screens
	> kubectl get pods -w

05-05-Kubernetes-Namespaces
05-05-02-Namespaces-LimitRange-default
https://github.com/stacksimplify/aws-eks-kubernetes-masterclass/tree/master/05-Kubernetes-Important-Concepts-for-Application-Deployments/05-05-Kubernetes-Namespaces/05-05-02-Namespaces-LimitRange-default
- Important note: File name starts with 00- so that when creating k8s objects namespaces will get created first
- Instead of specifying resources like cpu and memory in every container spec o a pod definition,
	- we can provide the default CPU & Memory for all containers in a namespace using Limit Range
	- Kind: ResourceQuota
- Get & Describe Limits
	> kubectl get limits -n dev3
	> kubectl describe limits default-cpu-mem-limit-range -n dev3

05-05-03-Namespaces-ResourceQuota
https://github.com/stacksimplify/aws-eks-kubernetes-masterclass/tree/master/05-Kubernetes-Important-Concepts-for-Application-Deployments/05-05-Kubernetes-Namespaces/05-05-03-Namespaces-ResourceQuota
- Get & Describe Limits
	> kubectl get limits -n dev3
	> kubectl describe limits default-cpu-mem-limit-range -n dev3
- Get Resource Quota
	> kubectl get quota -n dev3
	> kubectl describe quota ns-resource-quota -n dev3

06-EKS-Storage-with-RDS-Database
https://github.com/stacksimplify/aws-eks-kubernetes-masterclass/tree/master/06-EKS-Storage-with-RDS-Database
- Create mysql externalName Service
- Connect to RDS database using kubectl
	> kubectl run -it --rm --image=mysql:5.7.22 --restart=Never mysql-client -- mysql -h usermgmtdb.c7hldelt9xfp.us-east-1.rds.amazonaws.com -u dbadmin -pdbpassword11

07-ELB-Classic-and-Network-LoadBalancers
07-01-Create-EKS-Private-NodeGroup
https://github.com/stacksimplify/aws-eks-kubernetes-masterclass/tree/master/07-ELB-Classic-and-Network-LoadBalancers/07-01-Create-EKS-Private-NodeGroup
- Create Private Node Group in a Cluster
- Key option for the command is --node-private-networking
	> eksctl create nodegroup --cluster=eksdemo1 \
                        --region=us-east-1 \
                        --name=eksdemo1-ng-private1 \
                        --node-type=t3.medium \
                        --nodes-min=2 \
                        --nodes-max=4 \
                        --node-volume-size=20 \
                        --ssh-access \
                        --ssh-public-key=kube-demo \
                        --managed \
                        --asg-access \
                        --external-dns-access \
                        --full-ecr-access \
                        --appmesh-access \
                        --alb-ingress-access \
                        --node-private-networking
- Subnet Route Table Verification - Outbound Traffic goes via NAT Gateway
	- Verify the node group subnet routes to ensure it created in private subnets
	- Go to Services -> EKS -> eksdemo -> eksdemo1-ng1-private
	- Click on Associated subnet in Details tab
	- Click on Route Table Tab.
	- We should see that internet route via NAT Gateway (0.0.0.0/0 -> nat-xxxxxxxx)

07-03-Network-LoadBalancer-NLB
https://github.com/stacksimplify/aws-eks-kubernetes-masterclass/tree/master/07-ELB-Classic-and-Network-LoadBalancers/07-03-Network-LoadBalancer-NLB

